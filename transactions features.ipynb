{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume that we save all files in all/ folder in the same directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utils \n",
    "def lag_categorize_feature(data, prefix = ''):\n",
    "    if data > 0: \n",
    "        if data > 5:\n",
    "            return ('highly_decreasing{}'.format(prefix))\n",
    "        else:\n",
    "            return ('decreasing{}'.format(prefix))\n",
    "    elif data == 0 :\n",
    "        return ('stable{}'.format(prefix))\n",
    "    elif data < 0 :\n",
    "        if data < -5: \n",
    "            return ('highly_increasing{}'.format(prefix))\n",
    "        else: \n",
    "            return ('increasing{}'.format(prefix))\n",
    "    else: \n",
    "        return ('No_info{}'.format(prefix))\n",
    "    \n",
    "def weekday_weekend_clf(data):\n",
    "    if data in (5, 6):\n",
    "        return 'weekend'\n",
    "    else:\n",
    "        return 'weekday'\n",
    "    \n",
    "    \n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "# Insert whatever audio file you want above    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_train(folder):\n",
    "    df = pd.read_csv('all/train.csv')\n",
    "    df['fdt'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['yr'] = df.fdt.apply(lambda x: x.year)\n",
    "    df['tenure'] = ((datetime.date(2018, 2, 1) - df['fdt'].dt.date).dt.days)/30\n",
    "    return df\n",
    "\n",
    "def read_test(folder):\n",
    "    df = pd.read_csv('all/test.csv')\n",
    "    df['fdt'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['yr'] = df.fdt.apply(lambda x: x.year)\n",
    "    df['tenure'] = ((datetime.date(2018, 2, 1) - df['fdt'].dt.date).dt.days)/30\n",
    "    return df\n",
    "\n",
    "def read_txns(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    \n",
    "    # convert to date time\n",
    "    df['purchase_date'] = pd.to_datetime(df.purchase_date)\n",
    "#     df['purchase_weekday'] = map(weekday_weekend_clf, df.purchase_date.dt.weekday)    \n",
    "    df['purchase_weekday'] = df.purchase_date.dt.weekday\n",
    "    df['purchase_month'] = df['purchase_date'].dt.month\n",
    "    df['purchase_firstday'] = df['purchase_date'].dt.day==1\n",
    "    # binary \n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "    \n",
    "def read_merchant(fname):\n",
    "    # Since category column exist in transaction data already, we filter them out before merge into transaction data \n",
    "    merchant = pd.read_csv(fname).drop(['category_1', 'category_2', 'city_id', 'state_id', 'merchant_category_id', 'subsector_id'], 1)\n",
    "    merchant['sales_lag_3'] = map(lag_categorize_feature, merchant.avg_sales_lag3.values)\n",
    "    merchant['sales_lag_6'] = map(lag_categorize_feature, merchant.avg_sales_lag6.values)\n",
    "    merchant['sales_lag_12'] = map(lag_categorize_feature, merchant.avg_sales_lag12.values)\n",
    "    merchant['purchases_lag3'] = map(lag_categorize_feature, merchant.avg_purchases_lag3.values)\n",
    "    merchant['purchases_lag6'] = map(lag_categorize_feature, merchant.avg_purchases_lag6.values)\n",
    "    merchant['purchases_lag12'] = map(lag_categorize_feature, merchant.avg_purchases_lag12.values)\n",
    "    merchant = pd.get_dummies(merchant, columns= ['sales_lag_3', \n",
    "                                                  'sales_lag_6', \n",
    "                                                  'sales_lag_12', \n",
    "                                                  'purchases_lag3', \n",
    "                                                  'purchases_lag6', \n",
    "                                                  'purchases_lag12',\n",
    "                                                  'most_recent_sales_range', \n",
    "                                                  'most_recent_purchases_range'])\n",
    "    return merchant \n",
    "\n",
    "def agg_tx_features(df, prefix=\"\"):\n",
    "    df.purchase_weekday = map(weekday_weekend_clf, df.purchase_weekday)    \n",
    "    df =  pd.get_dummies(df, columns=['purchase_weekday'\n",
    "                                       ,'category_1'\n",
    "                                       , 'category_2'\n",
    "                                       , 'category_3'])\n",
    "    agg = df.groupby(by='card_id').agg(\n",
    "        {'purchase_amount': ['count','sum','mean','min','max', 'var', 'skew']\n",
    "         ,'merchant_id': ['nunique']\n",
    "         ,'installments': ['sum']\n",
    "         ,'authorized_flag': ['mean']\n",
    "         ,'category_1_0': ['count', 'mean']\n",
    "         ,'category_1_1': ['count', 'mean']         \n",
    "         ,'category_2_1.0': ['count', 'mean']\n",
    "         ,'category_2_2.0': ['count', 'mean']\n",
    "         ,'category_2_3.0': ['count', 'mean']\n",
    "         ,'category_2_4.0': ['count', 'mean']\n",
    "         ,'category_2_5.0': ['count', 'mean']\n",
    "         ,'category_3_A': ['count', 'mean']\n",
    "         ,'category_3_B': ['count', 'mean']\n",
    "         ,'category_3_C': ['count', 'mean']\n",
    "         ,'state_id': ['nunique']\n",
    "         ,'city_id': ['nunique']\n",
    "         ,'purchase_month': ['nunique']\n",
    "         ,'month_lag': ['nunique', 'min', 'max']\n",
    "         ,'purchase_firstday': ['sum', 'mean']  \n",
    "         , 'merchant_category_id': ['nunique']\n",
    "         , 'most_recent_sales_range_A': ['sum', 'mean']\n",
    "         , 'most_recent_sales_range_B': ['sum', 'mean']\n",
    "         , 'most_recent_sales_range_C': ['sum', 'mean']\n",
    "         , 'most_recent_sales_range_D': ['sum', 'mean']\n",
    "         , 'most_recent_sales_range_E': ['sum', 'mean']\n",
    "         , 'most_recent_purchases_range_A' : ['sum', 'mean']\n",
    "         , 'most_recent_purchases_range_B' : ['sum', 'mean']\n",
    "         , 'most_recent_purchases_range_C' : ['sum', 'mean']\n",
    "         , 'most_recent_purchases_range_D' : ['sum', 'mean']\n",
    "         , 'most_recent_purchases_range_E' : ['sum', 'mean']         \n",
    "         , 'purchases_lag3_highly_decreasing' : ['sum']\n",
    "         , 'purchases_lag3_decreasing' : ['sum']\n",
    "#          , 'purchases_lag3_stable' : ['sum']\n",
    "#          , 'purchases_lag3_highly_increasing' : ['sum']\n",
    "#          , 'purchases_lag3_increasing' : ['sum']\n",
    "\n",
    "         , 'purchases_lag6_highly_decreasing' : ['sum']\n",
    "         , 'purchases_lag6_decreasing' : ['sum']\n",
    "#          , 'purchases_lag6_stable' : ['sum']\n",
    "#          , 'purchases_lag6_highly_increasing' : ['sum']\n",
    "#          , 'purchases_lag6_increasing' : ['sum']\n",
    "         \n",
    "         , 'purchases_lag12_highly_decreasing' : ['sum']\n",
    "         , 'purchases_lag12_decreasing' : ['sum']\n",
    "#          , 'purchases_lag12_stable' : ['sum']\n",
    "#          , 'purchases_lag12_highly_increasing' : ['sum']\n",
    "#          , 'purchases_lag12_increasing' : ['sum']         \n",
    "        }).reset_index() \n",
    "    \n",
    "    agg.columns = [\"card_id\"] + [ prefix + '_'.join(tup).rstrip('_') \\\n",
    "                                 for tup in agg.columns.values[1:]]\n",
    "    return agg\n",
    "\n",
    "def combine_card_with_transaction(card, txs):\n",
    "    # combine_card_with_transaction\n",
    "    combine_data = card.merge(txs, how='left', on='card_id')\n",
    "    combine_data['no_new_tx'] = combine_data.all_purchase_amount_min.isnull()\n",
    "    combine_data['active_pre_newtx'] = combine_data.fdt <= txs.purchase_date.min()\n",
    "    \n",
    "\n",
    "    combine_data = combine_data.fillna(0)\n",
    "    agg_features = list(txs.columns.values)\n",
    "    agg_features.remove('card_id')\n",
    "    feat =['yr', 'tenure', 'no_new_tx', 'active_pre_newtx'] + agg_features\n",
    "    \n",
    "    # X and y (if exist)\n",
    "    X = combine_data[feat]\n",
    "    try: \n",
    "        y = combine_data.target\n",
    "    except: \n",
    "        print(\"No target data, return nan as y.\")\n",
    "        y = np.nan\n",
    "        \n",
    "        \n",
    "    # Return with dict \n",
    "    result = {'card_with_txs': agg_features\n",
    "             , 'featurs': feat\n",
    "             , 'X': X\n",
    "             , 'y': y\n",
    "             }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate(y, pred):\n",
    "    res = pd.DataFrame({'y':y, 'pred':pred})\n",
    "    res['diff'] = res.y-res.pred\n",
    "    res['ab_diff'] = res['diff'].abs()\n",
    "    res['diff_sq'] = res['diff']**2\n",
    "    res['is_outlier'] = res.y < -33\n",
    "    rmse = np.sqrt(np.mean(res['diff_sq']))\n",
    "    summary = res.groupby('is_outlier').agg({\n",
    "        'ab_diff': ['mean', 'min','max', 'median'],\n",
    "        'y':['count']\n",
    "    })\n",
    "    \n",
    "    return rmse, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a75c7298e9eee111fa7970c05db1e47692405681"
   },
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "90499e1748bfe695762f3824548400c2aaef8018",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cards = read_train('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_txs = read_txns('all/new_merchant_transactions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_txs = read_txns('all/historical_transactions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant = read_merchant('all/merchants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine h_txs + n_txs\n",
    "txs = h_txs.append(n_txs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del n_txs \n",
    "del h_txs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine merchant with transaction data \n",
    "txs_with_merchant = pd.merge(txs, merchant, how='left', on = 'merchant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "txs_with_merchant.to_csv(\"txs_with_merchant.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc49a614ab8906d8c81db39e472c9bc79ae3e768"
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txs_with_merchant = pd.read_csv('txs_with_merchant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e454d8437191d07626e030ff1c21a63d7acd4877"
   },
   "outputs": [],
   "source": [
    "agg_new = agg_tx_features(txs_with_merchant, \"all_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b014fad1909ecf05e5d2da7747e6e40157e5a321"
   },
   "source": [
    "# Combine aggregate transactions <>  training dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with RandomforestRegressor & grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_agg = combine_card_with_transaction(test, agg_new)\n",
    "X = test_agg['X']\n",
    "y = test_agg['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, Y\n",
    "y = c1.target\n",
    "X = c1[feat]\n",
    "\n",
    "# tr, val = train_test_split(train_df, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.train(tr)\n",
    "clf = RandomForestRegressor(max_depth=10, n_estimators=5000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Grid search params\n",
    "# clf_cv = GridSearchCV(clf, {'max_depth': [5, 10], 'n_estimators': [3000, 5000]}, verbose=1, cv = 2)\n",
    "# clf_cv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7c4d59648fbd4a6d827c1093233ac75e615d31a"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "perf = evaluate(y_test,y_pred)\n",
    "\n",
    "print('rmse:' + str(perf[0]))\n",
    "print(perf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "991ca534da41162d10432fa2aa573ff69eb856e9"
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with xgb & grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:3.84222694653\n",
      "                y    ab_diff                                 \n",
      "            count       mean        min        max     median\n",
      "is_outlier                                                   \n",
      "False       39919   1.279830   0.000012  16.868326   0.901875\n",
      "True          465  31.656774  27.702533  34.174740  31.848056\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBRegressor()\n",
    "# Grid search params\n",
    "# reg_cv = GridSearchCV(gbm, {\"colsample_bytree\":[1.0],\"min_child_weight\":[1.0]\n",
    "#                             ,'max_depth': [5,10], 'n_estimators': [3000, 5000]}, verbose=1, cv = 2)\n",
    "# reg_cv.fit(X_train,y_train)\n",
    "# gbm = xgb.XGBRegressor(**reg_cv.best_params_) # input best params\n",
    "gbm.fit(X_train,y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "perf = evaluate(y_test,y_pred)\n",
    "\n",
    "print('rmse:' + str(perf[0]))\n",
    "print(perf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_month_lag_min</th>\n",
       "      <td>0.141618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure</th>\n",
       "      <td>0.132948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_month_lag_max</th>\n",
       "      <td>0.117052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_authorized_flag_mean</th>\n",
       "      <td>0.109827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_1_1_mean</th>\n",
       "      <td>0.050578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_month_lag_nunique</th>\n",
       "      <td>0.034682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_installments_sum</th>\n",
       "      <td>0.030347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_skew</th>\n",
       "      <td>0.027457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_min</th>\n",
       "      <td>0.023121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_max</th>\n",
       "      <td>0.023121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_merchant_category_id_nunique</th>\n",
       "      <td>0.021676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_firstday_mean</th>\n",
       "      <td>0.020231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_firstday_sum</th>\n",
       "      <td>0.018786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>0.018786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_month_nunique</th>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_A_mean</th>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_var</th>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_C_mean</th>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_merchant_id_nunique</th>\n",
       "      <td>0.013006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchases_lag12_decreasing_sum</th>\n",
       "      <td>0.011561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_2.0_count</th>\n",
       "      <td>0.011561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_mean</th>\n",
       "      <td>0.010116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_D_mean</th>\n",
       "      <td>0.010116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_B_mean</th>\n",
       "      <td>0.010116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_A_mean</th>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_C_mean</th>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_2.0_mean</th>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_A_sum</th>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_E_mean</th>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_3.0_mean</th>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchases_lag6_highly_decreasing_sum</th>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_D_mean</th>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_A_mean</th>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_E_sum</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_E_sum</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_4.0_mean</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_city_id_nunique</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_D_sum</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_1.0_mean</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_B_mean</th>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_5.0_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchases_lag12_highly_decreasing_sum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchases_lag6_decreasing_sum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_state_id_nunique</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_pre_newtx</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_C_sum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_new_tx</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_B_mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_1_1_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_1_0_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_1_0_mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_purchases_range_B_sum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_most_recent_sales_range_E_mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_4.0_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_purchase_amount_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_A_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_B_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_3_C_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_1.0_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_category_2_3.0_count</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           importance\n",
       "all_month_lag_min                            0.141618\n",
       "tenure                                       0.132948\n",
       "all_month_lag_max                            0.117052\n",
       "all_authorized_flag_mean                     0.109827\n",
       "all_category_1_1_mean                        0.050578\n",
       "all_month_lag_nunique                        0.034682\n",
       "all_installments_sum                         0.030347\n",
       "all_purchase_amount_skew                     0.027457\n",
       "all_purchase_amount_min                      0.023121\n",
       "all_purchase_amount_max                      0.023121\n",
       "all_merchant_category_id_nunique             0.021676\n",
       "all_purchase_firstday_mean                   0.020231\n",
       "all_purchase_firstday_sum                    0.018786\n",
       "yr                                           0.018786\n",
       "all_purchase_month_nunique                   0.014451\n",
       "all_most_recent_purchases_range_A_mean       0.014451\n",
       "all_purchase_amount_var                      0.014451\n",
       "all_category_3_C_mean                        0.014451\n",
       "all_merchant_id_nunique                      0.013006\n",
       "all_purchases_lag12_decreasing_sum           0.011561\n",
       "all_category_2_2.0_count                     0.011561\n",
       "all_purchase_amount_mean                     0.010116\n",
       "all_most_recent_purchases_range_D_mean       0.010116\n",
       "all_category_3_B_mean                        0.010116\n",
       "all_most_recent_sales_range_A_mean           0.008671\n",
       "all_most_recent_purchases_range_C_mean       0.008671\n",
       "all_category_2_2.0_mean                      0.007225\n",
       "all_most_recent_purchases_range_A_sum        0.007225\n",
       "all_most_recent_purchases_range_E_mean       0.007225\n",
       "all_category_2_3.0_mean                      0.007225\n",
       "...                                               ...\n",
       "all_purchases_lag6_highly_decreasing_sum     0.002890\n",
       "all_most_recent_sales_range_D_mean           0.002890\n",
       "all_category_3_A_mean                        0.002890\n",
       "all_most_recent_purchases_range_E_sum        0.001445\n",
       "all_most_recent_sales_range_E_sum            0.001445\n",
       "all_category_2_4.0_mean                      0.001445\n",
       "all_city_id_nunique                          0.001445\n",
       "all_most_recent_sales_range_D_sum            0.001445\n",
       "all_category_2_1.0_mean                      0.001445\n",
       "all_most_recent_sales_range_B_mean           0.001445\n",
       "all_category_2_5.0_count                     0.000000\n",
       "all_purchases_lag12_highly_decreasing_sum    0.000000\n",
       "all_purchases_lag6_decreasing_sum            0.000000\n",
       "all_state_id_nunique                         0.000000\n",
       "active_pre_newtx                             0.000000\n",
       "all_most_recent_purchases_range_C_sum        0.000000\n",
       "no_new_tx                                    0.000000\n",
       "all_most_recent_purchases_range_B_mean       0.000000\n",
       "all_category_1_1_count                       0.000000\n",
       "all_category_1_0_count                       0.000000\n",
       "all_category_1_0_mean                        0.000000\n",
       "all_most_recent_purchases_range_B_sum        0.000000\n",
       "all_most_recent_sales_range_E_mean           0.000000\n",
       "all_category_2_4.0_count                     0.000000\n",
       "all_purchase_amount_count                    0.000000\n",
       "all_category_3_A_count                       0.000000\n",
       "all_category_3_B_count                       0.000000\n",
       "all_category_3_C_count                       0.000000\n",
       "all_category_2_1.0_count                     0.000000\n",
       "all_category_2_3.0_count                     0.000000\n",
       "\n",
       "[69 rows x 1 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(gbm.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict for new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = read_test('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### combine txs <> test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg = combine_card_with_transaction(test, agg_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_agg['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv(\"y_pred_rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
